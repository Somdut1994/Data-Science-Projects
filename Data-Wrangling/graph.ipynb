{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e9be088",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1233767",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from static_grader import grader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083b4db6",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "# The New York Social Graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f02b8f7",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "[New York Social Diary](https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/) provides a\n",
    "fascinating lens onto New York's socially well-to-do.  The data forms a natural [social graph](https://en.wikipedia.org/wiki/Social_graph) for New York's social elite.  Take a look at this page of a recent [run-of-the-mill holiday party](https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures/2014/holiday-dinners-and-doers).\n",
    "\n",
    "Besides the brand-name celebrities, you will notice the photos have carefully annotated captions labeling those that appear in the photos.  We can think of this as implicitly implying a social graph: there is a connection between two individuals if they appear in a picture together.\n",
    "\n",
    "For this project, we will assemble the social graph from photo captions for parties _dated December 1, 2014 and before_.  Using this graph, we can make guesses at the most popular socialites, the most influential people, and the most tightly coupled pairs.\n",
    "\n",
    "These pages are hosted on the Internet Archive, which can be quite slow and unreliable. To get around this, we have created an API that provides the captions. This API lives at `https://party-captions.tditrain.com`. The [documentation](https://party-captions.tditrain.com) describes how this API works in detail. At a high level, it's divided into two parts\n",
    "- An endpoint that provides a list of parties, `/parties`\n",
    "- An endpoint that provides the captions for a given party, `/captions`\n",
    "\n",
    "Both take parameters that allow us to select what we're looking for.\n",
    "\n",
    "To get the social graph that we want, we'll attack the problem in several steps:\n",
    "1. Get a list of the parties we want\n",
    "1. Parse the names from each caption for one party\n",
    "1. Parse the names for the rest of the parties\n",
    "1. Assemble the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca67250",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Getting the parties\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e0b4d",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The `/parties` endpoint provides us with a list of party names and dates (the date the party occurred). It can only provide up to 100 at a time, and there are over 1000 parties in the data set. By using the `limit` and `offset` parameters, as described in the documentation, get a list of all of the parties and their dates.\n",
    "\n",
    "As we did in class, we recommend using [`requests`](http://docs.python-requests.org/en/master/) to hit the endpoint. The checkpoints are expecting a list where each element corresponds to one party. How you want to represent this party (as a tuple, a dictionary, or something else) is up to you.\n",
    "\n",
    "The API will return a JSON object containing a list of party names and dates, and some metadata.  Here is an example, only returning the first ten parties for our convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3b26c51",
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': '',\n",
       " 'next': 'parties?offset=10&limit=10',\n",
       " 'parties': [{'date': '2015-03-13',\n",
       "   'name': '2015-bunny-hop-the-boys-club-old-bags-and-more'},\n",
       "  {'date': '2015-03-11',\n",
       "   'name': '2015-sab-the-jewish-museum-roundabout-theatre-and-faces'},\n",
       "  {'date': '2015-03-05',\n",
       "   'name': '2015-adaa-art-show-bronx-museum-the-china-arts-foundation-international-and-the-palm'},\n",
       "  {'date': '2015-03-04',\n",
       "   'name': '2015-the-60th-anniversary-of-the-viennese-opera-ball-and-mcnys-directors-council'},\n",
       "  {'date': '2015-03-02',\n",
       "   'name': '2015-the-new-york-philharmonic-the-new-york-botanical-garden-longhouse-reserve-and'},\n",
       "  {'date': '2015-02-26', 'name': '2015-mission-accomplished'},\n",
       "  {'date': '2015-02-25', 'name': '2015-philanthropic-endeavors'},\n",
       "  {'date': '2015-02-23', 'name': '2015-dining-with-the-divas'},\n",
       "  {'date': '2015-02-18', 'name': '2015-fielding-dreams'},\n",
       "  {'date': '2015-02-11', 'name': '2015-generosity-leadership'}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "requests.get(\"https://party-captions.tditrain.com/parties?limit=10\").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3022ea0f",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We want the information in the `parties` element. You will need to call the API multiple times to get all the parties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50469916",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "party_list_all=[]\n",
    "req_num=0\n",
    "while True:\n",
    "    message_map=requests.get(f\"https://party-captions.tditrain.com/parties?limit=100&offset={req_num*100}\").json()\n",
    "    if len(message_map['parties'])==0:\n",
    "        break\n",
    "    req_num+=1\n",
    "    party_list_all.extend(message_map['parties'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713b0ffb",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Now that we have our list of parties, we'll need to remove those that occurred after December 1st, 2014 (we keep the ones that occurred _on_ or before that date). The API provided us with the dates, as strings. One option would be to use `datetime`'s `strptime` method and the [format codes for dates](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior) to parse this into dates for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0577d7b6",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "party_list =[]\n",
    "for item in party_list_all:\n",
    "    if item['date']>'2014-12-01':\n",
    "        continue\n",
    "    party_list.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f3b95f9",
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you have successfully gotten all of the parties, there should be 1145 of them\n",
    "# Double check that you are not skipping or duplicating parties\n",
    "# if you are, look at how you are incrementing your offset\n",
    "# Have you filtered by the date?\n",
    "grader.check(len(party_list) == 1145)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1395d0c3",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "To avoid having to get the party list from the API again if we restart the notebook, we should save this list to a file. There are many ways to do it, here's how with `dill`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f749ed66",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "with open('nysd-parties.pkd', 'wb') as f:\n",
    "    dill.dump(party_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3045ba",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "And to read it back later, we just `load` it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fce5d594",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "with open('nysd-parties.pkd', 'rb') as f:\n",
    "    party_list = dill.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc795332",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 1: histogram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ea826f",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Get the number of party pages for each of the 95 months (that is, month-year pair) in the data.  Represent this histogram as a list of 95 tuples, each of the form `(\"Dec-2014\", 1)`.  Note that you can convert `datetime` objects into these sort of strings with `strftime` and the [format codes for dates](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior) from before.\n",
    "\n",
    "The grader is expecting the list of tuples. \n",
    "\n",
    "Plot the histogram for yourself.  Do you see any trends?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fecc37e1",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "monthCount=defaultdict(int)\n",
    "from datetime import datetime\n",
    "\n",
    "for party in party_list:\n",
    "    monthCount[datetime.strptime(party['date'], '%Y-%m-%d').strftime('%b-%Y')]+=1\n",
    "\n",
    "histogram=[]\n",
    "for item in monthCount:\n",
    "    histogram.append((item, monthCount[item]))\n",
    "    \n",
    "\n",
    "#histogram = [(\"Dec-2014\", 1)] * 95  # Replace this fake answer with your real results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ba338eb",
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 1.0000\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "grader.score('graph__histogram', histogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f2509d",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Parsing captions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba425b3",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We now have all of the parties.  For each party, we'll need to get the captions, then find who appears in each caption. Let's start with a single party, [the benefit cocktails and dinner](https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures/2015/celebrating-the-neighborhood) for [Lenox Hill Neighborhood House](http://www.lenoxhill.org/), a neighborhood organization for the East Side. In our API, this corresponds to the party named `2015-celebrating-the-neighborhood`.  Let's get the captions for it.\n",
    "\n",
    "In our API, the `/captions` endpoint takes a parameter `party`, which we give the name of the party we want. We can then extract the captions from the JSON it returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba4e6c5f",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "response = requests.get('https://party-captions.tditrain.com/captions?party=2015-celebrating-the-neighborhood')\n",
    "captions = response.json()['captions']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d16c803",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We'll need to do this for all of our parties later, so we should make it a function we can call. Take the party name as an argument and return the list of captions. \n",
    "\n",
    "We want to avoid having to hit the API repeatedly the next time we need to run the notebook.  While you could save the files by hand, as we did before, a checkpoint library like [ediblepickle](https://pypi.python.org/pypi/ediblepickle/1.1.3) can handle this for you.  (Note, though, that you may not want to enable this until you are sure that your function is working.)\n",
    "\n",
    "You should also keep in mind that HTTP requests fail occasionally, for transient reasons.  You should plan how to detect and react to these failures.   The [retrying module](https://pypi.python.org/pypi/retrying) is one way to deal with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fba86a8",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "def get_captions(party_name):\n",
    "    return requests.get(f'https://party-captions.tditrain.com/captions?party={party_name}').json()['captions']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c9e948",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "If things have gone according to plan, this should get the same captions as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "119326b9",
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cell is expecting get_captions to return a list of the captions themselves\n",
    "# Other routes to a solution might need to adjust this cell a bit\n",
    "grader.check(get_captions('2015-celebrating-the-neighborhood') == captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeda863",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Now that we have some sample captions, let's start parsing names out of those captions.  There are many ways of going about this, and we leave the details up to you.  Some issues to consider:\n",
    "\n",
    "  1. Some captions are not useful: they contain long narrative texts that explain the event.  Try to find some heuristic rules to separate captions that are a list of names from those that are not.  A few heuristics include:\n",
    "    - Look for sentences (which have verbs) and as opposed to lists of nouns. For example, [`nltk` does part of speech tagging](http://www.nltk.org/book/ch05.html) but it is a little slow. There may also be heuristics that accomplish the same thing.\n",
    "    - Similarly, spaCy's [entity recognition](https://spacy.io/docs/usage/entity-recognition) could be useful here, but like `nltk` using `spaCy` will add to processing time.\n",
    "    - Look for commonly repeated threads (e.g. you might end up picking up the photo credits or people such as \"a friend\").\n",
    "    - Long captions are often not lists of people.  The cutoff is subjective, but for grading purposes **we set that cutoff at 250 characters**.\n",
    "  1. Many of the captions contain extraneous whitespace or other formatting issues you may need to deal with.\n",
    "  1. You will want to separate the captions based on various forms of punctuation.  Try using `re.split`, which is more sophisticated than `string.split`. **Note**: The reference solution uses regex exclusively for name parsing.\n",
    "  1. You might find a person named \"ra Lebenthal\".  There is no one by this name.  Any idea what might cause that?\n",
    "  1. This site is pretty formal and likes to say things like \"Mayor Michael Bloomberg\" after his election but \"Michael Bloomberg\" before his election.  Can you find other ('optional') titles that are being used?  They should probably be filtered out because they ultimately refer to the same person: \"Michael Bloomberg.\"\n",
    "  1. There is a special case you might find where couples are written as e.g. \"John and Mary Smith\". You will need to write some extra logic to make sure this properly parses to two names: \"John Smith\" and \"Mary Smith\".\n",
    "  1. When parsing names from captions, it can help to look at your output frequently and address the problems that you see coming up, iterating until you have a list that looks reasonable. This is the approach used in the reference solution. Because we can only asymptotically approach perfect identification and entity matching, we have to stop somewhere.\n",
    "  1. Your eye is very good at doing this sort of parsing.  You will find it helpful to look at a caption and the names you parse of out it. Do this for a selection of captions to detect potential issues.\n",
    "  1. You want to keep the names in a caption together - that's how we can tell they're connected to each other! You should get one list of names for each caption.\n",
    "  \n",
    "**Questions worth considering:**\n",
    "  1. Who is Patrick McMullan and should he be included in the results? How would you address this?\n",
    "  2. What else could you do to improve the quality of the graph's information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "880aa722",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "# You will want to make a function that takes in a caption and returns a list of names\n",
    "import re\n",
    "\n",
    "def checkName(name):\n",
    "    # Too many small letter words in a name, von, van, de, etc. are okay\n",
    "    nameparts=name.split()\n",
    "    i=0\n",
    "    for part in nameparts:\n",
    "        if ord(part[0])>=ord('a') and ord(part[0])<=ord('z'):\n",
    "            i+=1\n",
    "    if len(nameparts)>=2:\n",
    "        return (i<=1)\n",
    "    return (i==0)\n",
    "        \n",
    "def getnames(caption):\n",
    "    res=[]\n",
    "    if len(caption)<=250:\n",
    "        nameList=re.split(', and |, | with ', caption)\n",
    "        for name in nameList:\n",
    "            name=re.split(' as| at| on| \\(| &| in', name)[0]\n",
    "            name=re.split('Mr. |Mrs. |Mayor |Dr. ', name)[-1]\n",
    "            if len(name.split())>0 and len(name.split(\"'\"))==1 and len(name.split('\"'))==1 and len(name.split('The'))==1:\n",
    "                if len(name.split(' and '))>1:\n",
    "                    names=[' '.join(split_name.split()) for split_name in name.split(' and ')]\n",
    "                    if len(names[0].split(' '))<2 and len(names[1].split(' '))==2:\n",
    "                        names[0]+=(' '+names[1].split(' ')[1])\n",
    "                    for split_name in names:\n",
    "                        if checkName(split_name):\n",
    "                            res.append(split_name)  \n",
    "                else:\n",
    "                    if checkName(name):\n",
    "                        res.append(' '.join(name.split()))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb83977c",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 2: sample_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feaa4cd",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Once you feel that your algorithm is working well, parse all of the captions we got from the `2015-celebrating-the-neighborhood` party and extract all the names mentioned.  Sort them alphabetically, by first name, and return the first hundred unique names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6182893f",
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 1.0000\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "captions = get_captions('2015-celebrating-the-neighborhood')\n",
    "all_names=set()\n",
    "for caption in captions:\n",
    "    names=getnames(caption)\n",
    "    for name in names:\n",
    "        all_names.add(name)\n",
    "\n",
    "sample_names=sorted(list(all_names))[:100]\n",
    "\n",
    "grader.score('graph__sample_names', sample_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00d6a9f",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Now, test your tools on a few other parties.  You will probably find that other parties have new issues in their captions that trip up your caption parser.  But don't worry if the parser isn't perfect - just try to get the easy cases for now. You may need to come back and refine it more for the later questions, however."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39243634",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Parsing all the parties\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffcacb4",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Once you are satisfied that your parser is working, we want to run it for all of our parties. First, get the captions for all of the parties in our party list. If you haven't implemented some caching of the captions, you probably want to do this first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "62fb936f",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "# It may take several minutes to fetch all the captions\n",
    "captions=[]\n",
    "for party in party_list:\n",
    "    partyCaptions=get_captions(party['name'])\n",
    "    captions.extend(partyCaptions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aee9b1",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "And parse the names in each caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "faee9c98",
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "# You should have a list of names for each caption\n",
    "# Depending on how you set up your parser, this may take quite a while\n",
    "Names=[]\n",
    "for caption in captions:\n",
    "    Names.append(getnames(caption))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023a5f8c",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "You should end up with over 100,000 captions and roughly 110,000 names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cf21c0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238423 127127\n"
     ]
    }
   ],
   "source": [
    "print(sum([len(item) for item in Names]), len(captions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cadaace",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Building the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f7d985",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "For the remaining analysis, we think of the problem in terms of a\n",
    "[network](http://en.wikipedia.org/wiki/Computer_network) or a\n",
    "[graph](https://en.wikipedia.org/wiki/Graph_%28discrete_mathematics%29).  Any time a pair of people appear in a caption together, that is considered a link.  What we have described is more appropriately called an (undirected)\n",
    "[multigraph](http://en.wikipedia.org/wiki/Multigraph) with no self-loops, but this has an obvious analog in terms of an undirected [weighted graph](http://en.wikipedia.org/wiki/Graph_%28mathematics%29#Weighted_graph).\n",
    "\n",
    "In the remainder of this miniproject, we will analyze the social graph of the New York social elite.  We recommend using python's [`networkx`](https://networkx.github.io/) library to build this social graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "434ce7e0",
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique connections: 195070\n"
     ]
    }
   ],
   "source": [
    "import itertools  # itertools.combinations may be useful\n",
    "import networkx as nx\n",
    "\n",
    "weight_map=defaultdict(int)\n",
    "popularity_map=defaultdict(int)\n",
    "\n",
    "for nameList in Names:\n",
    "    for i in range(len(nameList)):\n",
    "        for j in range(i+1, len(nameList)):\n",
    "            if nameList[j]<nameList[i]:\n",
    "                weight_map[(nameList[j], nameList[i])]+=1\n",
    "            else:\n",
    "                weight_map[(nameList[i], nameList[j])]+=1\n",
    "                \n",
    "print('Total unique connections:', len(weight_map))\n",
    "for u, v in weight_map:\n",
    "    popularity_map[u]+=weight_map[(u, v)]\n",
    "    popularity_map[v]+=weight_map[(u, v)]\n",
    "\n",
    "G=nx.Graph()\n",
    "G.add_weighted_edges_from([(u, v, weight_map[(u, v)]) for u, v in weight_map])\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63c1eec",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "You should find you have roughly 200,000 distinct pairs of people appearing in photos together - corresponding to how many (weighted) edges there are in our graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97eb992",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 3: degree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65a849e",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The simplest question to ask is \"who is the most popular\"?  The easiest way to answer this question is to look at how many connections everyone has.  Return the top 100 people and their degree.  Remember that if an edge of the graph has weight 2, it counts for 2 in the degree.\n",
    "\n",
    "**Checkpoint:** Some aggregate stats on the solution:\n",
    "    \n",
    "    count:  100.0\n",
    "    mean:   202.2\n",
    "    std:     94.1\n",
    "    min:    132.0\n",
    "    25%:    146.8\n",
    "    50%:    169.5\n",
    "    75%:    212.0\n",
    "    max:    713.0\n",
    "    \n",
    "Note that these checkpoints are guidelines, you may not match them exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b363147b",
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 0.9300\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "import heapq  # Heaps are efficient structures for tracking the largest\n",
    "              # elements in a collection.  Use introspection to find the\n",
    "              # function you need.\n",
    "\n",
    "minHeap=[]\n",
    "for item in popularity_map:\n",
    "    if len(minHeap)<100:\n",
    "        heapq.heappush(minHeap, [popularity_map[item], item])\n",
    "    elif minHeap[0][0]<popularity_map[item]:\n",
    "        heapq.heappushpop(minHeap, [popularity_map[item], item])\n",
    "        \n",
    "degree = [(name, count) for count, name in minHeap]\n",
    "\n",
    "grader.score('graph__degree', degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df730faa",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 4: PageRank\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1133d5",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "A similar way to determine popularity is to look at their\n",
    "[PageRank](http://en.wikipedia.org/wiki/PageRank).  PageRank is used for web ranking and was originally\n",
    "[patented](http://patft.uspto.gov/netacgi/nph-Parser?patentnumber=6285999) by Google and is essentially the stationary distribution of a [Markov\n",
    "chain](http://en.wikipedia.org/wiki/Markov_chain) implied by the social graph. You can implement this yourself or use the version in `networkx`.\n",
    "\n",
    "Use 0.85 as the damping parameter so that there is a 15% chance of jumping to another vertex at random.\n",
    "\n",
    "**Checkpoint:** Some aggregate stats on the solution:\n",
    "\n",
    "\n",
    "    count:  100.000000\n",
    "    mean:     0.000193\n",
    "    std:      0.000080\n",
    "    min:      0.000130\n",
    "    25%:      0.000144\n",
    "    50%:      0.000169\n",
    "    75%:      0.000206\n",
    "    max:      0.000646\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "84544580",
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 0.9100\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "pagerank = [('Martha Stewart', 0.00019312108706213307)] * 100\n",
    "\n",
    "pr = nx.pagerank(G, alpha=0.85)\n",
    "\n",
    "minHeap=[]\n",
    "for item in pr:\n",
    "    if len(minHeap)<100:\n",
    "        heapq.heappush(minHeap, [pr[item], item])\n",
    "    elif minHeap[0][0]<pr[item]:\n",
    "        heapq.heappushpop(minHeap, [pr[item], item])\n",
    "        \n",
    "pagerank=[(name, rank) for rank, name in minHeap]\n",
    "\n",
    "grader.score('graph__pagerank', pagerank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c408b8b1",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 5: best_friends\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0f3a91",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Another interesting question is who tend to co-occur with each other.  Give us the 100 edges with the highest weights.\n",
    "\n",
    "Google these people and see what their connection is.  Can we use this to detect instances of infidelity?\n",
    "\n",
    "**Checkpoint:** Some aggregate stats on the solution:\n",
    "\n",
    "    count:  100.0\n",
    "    mean:    27.5\n",
    "    std:     16.9\n",
    "    min:     14.0\n",
    "    25%:     17.0\n",
    "    50%:     21.5\n",
    "    75%:     31.3\n",
    "    max:    122.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ec7987e1",
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 0.9000\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "best_friends = [(('Michael Kennedy', 'Eleanora Kennedy'), 41)] * 100\n",
    "\n",
    "minHeap=[]\n",
    "for u, v in weight_map:\n",
    "    if len(minHeap)<100:\n",
    "        heapq.heappush(minHeap, [weight_map[(u, v)], u, v])\n",
    "    elif minHeap[0][0]<weight_map[(u, v)]:\n",
    "        heapq.heappushpop(minHeap, [weight_map[(u, v)], u, v])\n",
    "        \n",
    "best_friends=[((friend1, friend2), weight) for weight, friend1, friend2 in minHeap]\n",
    "\n",
    "grader.score('graph__best_friends', best_friends)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef6fd99",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "*Copyright &copy; 2022 Pragmatic Institute. This content is licensed solely for personal use. Redistribution or publication of this material is strictly prohibited.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
